{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIkB6WKFvtDrLFJGlMo+md",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Learny0/house-prediction-using-deep-learning-models/blob/main/21_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEARNMORE MATSIKA R227549D\n",
        "\n",
        "HOUSE PRICE PREDICTIONS\n"
      ],
      "metadata": {
        "id": "i1LKY1zbhRo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, math, time, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import timm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n"
      ],
      "metadata": {
        "id": "byZwyVc8hRSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOUNTING THE GOOGLE DRIVE SO AS TO BE ABLE TO USE DATA SETS IN THE DRIVE"
      ],
      "metadata": {
        "id": "Irbi2RfwhVf6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVUKH1uxhOev"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPECIFYING PATH TO FIND THE DATASETS IM WORKING ON"
      ],
      "metadata": {
        "id": "7rHrXq8JhbEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Path Configuration ---\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/house_price_project\"\n",
        "CSV_PATH = f\"{DRIVE_BASE}/final_zimbabwe_property_listings_complete.csv\"\n",
        "IMAGE_ROOT = f\"{DRIVE_BASE}/images\"\n",
        "EMBED_DIR = f\"{DRIVE_BASE}/embeddings\"\n",
        "OUTPUT_DIR = f\"{DRIVE_BASE}/outputs\"\n",
        "\n",
        "os.makedirs(EMBED_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "58wY2y6iheEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING DATSET TO SEE HOW IT IS"
      ],
      "metadata": {
        "id": "lG0TUI8phgRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load Dataset ---\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Columns in dataset:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Quick preview\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "EMnw0G1Fhiw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS CODE DEFINES A DICTIONARY OF 21 PRE-DEFINED CNN/TRANSFORMER BACKBONES WITH THEIR PROPERTIES (SOURCE LIBRARY, INPUT IMAGE SIZE, ETC.), A FUNCTION TO LOAD EACH BACKBONE PRE-TRAINED AND REMOVE ITS FINAL CLASSIFICATION LAYER SO IT CAN BE USED AS A FEATURE EXTRACTOR, AND A DATASET CLASS FOR LOADING AND TRANSFORMING PROPERTY IMAGES, HANDLING MISSING OR INVALID IMAGE FILES BY RETURNING A BLACK IMAGE."
      ],
      "metadata": {
        "id": "09AnPpLNhlFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full list of available backbones (we will use this dictionary to iterate through 21 models)\n",
        "BACKBONES = {\n",
        "    # --- Classic CNN Architectures (torchvision) ---\n",
        "    # 2012: AlexNet\n",
        "    \"alexnet\": {\"source\":\"torch\", \"name\":\"alexnet\", \"img_size\":224, \"key\":\"alexnet\"},\n",
        "\n",
        "    # 2014: VGG & GoogLeNet (Inception-V1)\n",
        "    \"vgg16\": {\"source\":\"torch\", \"name\":\"vgg16\", \"img_size\":224, \"key\":\"vgg16\"},\n",
        "    \"vgg19\": {\"source\":\"torch\", \"name\":\"vgg19\", \"img_size\":224, \"key\":\"vgg19\"},\n",
        "    \"googlenet\": {\"source\":\"torch\", \"name\":\"googlenet\", \"img_size\":224, \"key\":\"googlenet\"},\n",
        "\n",
        "    # 2015: Inception-V3\n",
        "    \"inception_v3\": {\"source\":\"torch\", \"name\":\"inception_v3\", \"img_size\":299, \"key\":\"inception_v3\"},\n",
        "\n",
        "    # 2016: ResNet, WideResNet\n",
        "    \"resnet18\": {\"source\":\"torch\", \"name\":\"resnet18\", \"img_size\":224, \"key\":\"resnet18\"},\n",
        "    \"resnet34\": {\"source\":\"torch\", \"name\":\"resnet34\", \"img_size\":224, \"key\":\"resnet34\"},\n",
        "    \"resnet50\": {\"source\":\"torch\", \"name\":\"resnet50\", \"img_size\":224, \"key\":\"resnet50\"},\n",
        "    \"resnet101\": {\"source\":\"torch\", \"name\":\"resnet101\", \"img_size\":224, \"key\":\"resnet101\"},\n",
        "    \"resnext50_32x4d\": {\"source\":\"torch\", \"name\":\"resnext50_32x4d\", \"img_size\":224, \"key\":\"resnext50_32x4d\"}, # ResNeXt\n",
        "    \"wide_resnet50_2\": {\"source\":\"torch\", \"name\":\"wide_resnet50_2\", \"img_size\":224, \"key\":\"wide_resnet50_2\"},\n",
        "\n",
        "    # 2017: DenseNet\n",
        "    \"densenet121\": {\"source\":\"torch\", \"name\":\"densenet121\", \"img_size\":224, \"key\":\"densenet121\"},\n",
        "    \"densenet161\": {\"source\":\"torch\", \"name\":\"densenet161\", \"img_size\":224, \"key\":\"densenet161\"},\n",
        "\n",
        "    # 2018: MobileNet-V2\n",
        "    \"mobilenet_v2\": {\"source\":\"torch\", \"name\":\"mobilenet_v2\", \"img_size\":224, \"key\":\"mobilenet_v2\"},\n",
        "\n",
        "    # Other torch models\n",
        "    \"shufflenet_v2_x1_0\": {\"source\":\"torch\", \"name\":\"shufflenet_v2_x1_0\", \"img_size\":224, \"key\":\"shufflenet_v2_x1_0\"},\n",
        "    \"mnasnet1_0\": {\"source\":\"torch\", \"name\":\"mnasnet1_0\", \"img_size\":224, \"key\":\"mnasnet1_0\"},\n",
        "\n",
        "    # --- Efficient Nets & Other Architectures (timm) ---\n",
        "    # 2013: NIN\n",
        "    \"nin\": {\"source\":\"timm\", \"name\":\"nin\", \"img_size\":224, \"key\":\"nin\"},\n",
        "\n",
        "    # 2016: Inception-V4, Inception-ResNet-V2\n",
        "    \"inception_v4\": {\"source\":\"timm\", \"name\":\"inception_v4\", \"img_size\":299, \"key\":\"inception_v4\"},\n",
        "    \"inception_resnet_v2\": {\"source\":\"timm\", \"name\":\"inception_resnet_v2\", \"img_size\":299, \"key\":\"inception_resnet_v2\"},\n",
        "\n",
        "    # 2017: Xception, SENet, Residual Attention (proxy: ResNeSt)\n",
        "    \"xception\": {\"source\":\"timm\", \"name\":\"xception\", \"img_size\":299, \"key\":\"xception\"},\n",
        "    \"se_resnext50_32x4d\": {\"source\":\"timm\", \"name\":\"se_resnext50_32x4d\", \"img_size\":224, \"key\":\"se_resnext50_32x4d\"}, # Proxy for SENet / Residual Attention\n",
        "\n",
        "    # 2020: HRNet-V2\n",
        "    \"hrnet_w18_small_v2\": {\"source\":\"timm\", \"name\":\"hrnet_w18_small_v2\", \"img_size\":224, \"key\":\"hrnet_w18_small_v2\"},\n",
        "\n",
        "    # EfficientNets (modern reference)\n",
        "    \"efficientnet_b0\": {\"source\":\"timm\", \"name\":\"efficientnet_b0\", \"img_size\":224, \"key\":\"efficientnet_b0\"},\n",
        "    \"efficientnet_b1\": {\"source\":\"timm\", \"name\":\"efficientnet_b1\", \"img_size\":240, \"key\":\"efficientnet_b1\"},\n",
        "    \"efficientnet_b3\": {\"source\":\"timm\", \"name\":\"efficientnet_b3\", \"img_size\":300, \"key\":\"efficientnet_b3\"},\n",
        "    \"efficientnet_b5\": {\"source\":\"timm\", \"name\":\"efficientnet_b5\", \"img_size\":456, \"key\":\"efficientnet_b5\"},\n",
        "\n",
        "    # Transformer Architectures (modern reference)\n",
        "    \"vit_base_patch16_224\": {\"source\":\"timm\", \"name\":\"vit_base_patch16_224\", \"img_size\":224, \"key\":\"vit_base_patch16_224\", \"pool_fn\":\"token\"}, # Vision Transformer\n",
        "    \"swin_tiny_patch4_window7_224\": {\"source\":\"timm\", \"name\":\"swin_tiny_patch4_window7_224\", \"img_size\":224, \"key\":\"swin_tiny_patch4_window7_224\"}, # Swin Transformer\n",
        "}\n",
        "\n",
        "def create_backbone(cfg, pretrained=True, device=DEVICE):\n",
        "    \"\"\"Initializes a pre-trained CNN and replaces the final classification layer with an identity layer.\"\"\"\n",
        "    name = cfg[\"name\"]\n",
        "    source = cfg.get(\"source\", \"timm\")\n",
        "\n",
        "    if source == \"torch\":\n",
        "        # Note: Using models.get_model_weights().IMAGENET1K_V1 for consistency with PyTorch recommended loading\n",
        "        m = getattr(models, name)(weights=models.get_model_weights(name).IMAGENET1K_V1)\n",
        "        if hasattr(m, \"fc\"):\n",
        "            featdim = m.fc.in_features\n",
        "            m.fc = nn.Identity()\n",
        "        elif hasattr(m, \"classifier\"):\n",
        "            try:\n",
        "                featdim = m.classifier.in_features\n",
        "                m.classifier = nn.Identity()\n",
        "            except:\n",
        "                featdim = m.classifier[-1].in_features\n",
        "                m.classifier = nn.Sequential(*list(m.classifier.children())[:-1])\n",
        "    else:\n",
        "        # Load timm models\n",
        "        global_pool = cfg.get(\"pool_fn\", \"avg\") # Use pool_fn from config if available (e.g. 'token' for ViT)\n",
        "        m = timm.create_model(name, pretrained=pretrained, num_classes=0, global_pool=global_pool)\n",
        "        featdim = getattr(m, \"num_features\", 2048)\n",
        "\n",
        "    m = m.to(device)\n",
        "    m.eval()\n",
        "    return m, featdim\n",
        "\n",
        "class PropertyImageDataset(Dataset):\n",
        "    \"\"\"Dataset for loading property images and applying transforms.\"\"\"\n",
        "    def __init__(self, df, image_root, image_col, img_size=224, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_root = Path(image_root)\n",
        "        self.image_col = image_col\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def _load_image(self, img_ref):\n",
        "        \"\"\"Attempts to load image, returns black image on failure.\"\"\"\n",
        "        if pd.isna(img_ref) or img_ref == \"\":\n",
        "            return Image.fromarray(np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8))\n",
        "\n",
        "        p = Path(img_ref)\n",
        "        # Try absolute path or path relative to image_root\n",
        "        for path_attempt in [p, self.image_root / str(img_ref), self.image_root / p.name]:\n",
        "            if path_attempt.is_file():\n",
        "                try: return Image.open(str(path_attempt)).convert(\"RGB\")\n",
        "                except: pass\n",
        "\n",
        "        # Fallback to a zero-filled (black) image\n",
        "        # print(f\"Warning: Image file not found for reference '{img_ref}'\")\n",
        "        return Image.fromarray(np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_ref = row.get(self.image_col, None)\n",
        "        pil = self._load_image(img_ref)\n",
        "        img = self.transform(pil)\n",
        "        return {\"image\": img, \"index\": idx}"
      ],
      "metadata": {
        "id": "MvuFJkf2hpJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS CODE LOADS THE PROPERTY CSV DATA (OR CREATES MOCK DATA IF CSV NOT FOUND), CLEANS AND PARSES MESSY PRICE AND SIZE STRINGS INTO NUMERIC VALUES, HANDLES NON-NUMERIC OR RANGE VALUES, IMPUTES MISSING NUMERICAL AND CATEGORICAL FEATURES, LOG-TRANSFORMS THE TARGET PRICE TO STABILIZE VARIANCE, AND SPLITS THE DATA INTO TRAIN, VALIDATION, AND TEST SETS, RETURNING INDEXES FOR CONSISTENT ACCESS DURING MODELING."
      ],
      "metadata": {
        "id": "zLaEN5mKhrWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import joblib # Import joblib for saving split indices\n",
        "\n",
        "# --- GLOBAL CONFIGURATION ---\n",
        "TARGET = \"price\"\n",
        "IMAGE_COL = \"image_filenames\"\n",
        "TAB_NUM_COLS = ['bedrooms', 'bathrooms', 'building_area', 'land_area']\n",
        "TAB_CAT_COLS = ['property_type', 'location']\n",
        "TARGET_LOG = 'price_clean_log' # This is the final target column name used in the code\n",
        "\n",
        "# --- 0. PATH CONFIGURATION (CONFIRMED AND CORRECTED) ---\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/house_price_project\"\n",
        "CSV_PATH = f\"{DRIVE_BASE}/final_zimbabwe_property_listings_complete.csv\"\n",
        "# üö© CONFIRMED: The output confirms that the image filenames are simple (e.g., 'image.jpg')\n",
        "# and reside directly inside the '/images' folder.\n",
        "IMAGE_ROOT = f\"{DRIVE_BASE}/images\"\n",
        "EMBED_DIR = f\"{DRIVE_BASE}/embeddings\"\n",
        "OUTPUT_DIR = f\"{DRIVE_BASE}/outputs\"\n",
        "\n",
        "# Create output directories if they don't exist (Optional, but good practice)\n",
        "os.makedirs(EMBED_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 1. DATA LOADING AND INITIAL FILTERING ---\n",
        "try:\n",
        "    # Use the full, absolute path to the CSV\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: CSV file not found at {CSV_PATH}. Please ensure your Drive is mounted and the path is correct.\")\n",
        "    # Exiting the script if the main data file is missing\n",
        "    sys.exit()\n",
        "\n",
        "print(f\"Original DataFrame size: {len(df)}\")\n",
        "\n",
        "# Filter 1: Drop rows where the image filename is missing or empty in the CSV\n",
        "initial_count = len(df)\n",
        "df = df[df[IMAGE_COL].notna() & (df[IMAGE_COL].astype(str).str.strip() != '')].copy()\n",
        "print(f\"Data size after filtering out properties with NO IMAGE FILENAME in CSV: {len(df)} (Dropped {initial_count - len(df)})\")\n",
        "print(\"Starting Data Preprocessing Pipeline...\")\n",
        "\n",
        "\n",
        "# --- 2. DEEP CLEANING AND TRANSFORMATION ---\n",
        "\n",
        "def clean_price(price_str):\n",
        "    \"\"\"Converts messy price strings to float, returns NaN if unparsable.\"\"\"\n",
        "    if pd.isna(price_str): return np.nan\n",
        "    price_str = str(price_str).strip()\n",
        "    if 'P.O.A' in price_str.upper() or price_str == '' or 'price on application' in price_str.lower():\n",
        "        return np.nan\n",
        "    price_str = price_str.replace('USD', '').replace('$', '').replace(',', '').strip()\n",
        "    try:\n",
        "        return float(price_str)\n",
        "    except ValueError:\n",
        "        if '-' in price_str:\n",
        "            try: return float(price_str.split('-')[0].strip())\n",
        "            except ValueError: return np.nan\n",
        "        return np.nan\n",
        "\n",
        "def clean_size(size_str):\n",
        "    \"\"\"Converts messy size strings (with units) to float, returns NaN if unparsable.\"\"\"\n",
        "    if pd.isna(size_str): return np.nan\n",
        "    size_str = str(size_str).replace('m¬≤', '').replace(',', '').strip()\n",
        "    try: return float(size_str)\n",
        "    except ValueError: return np.nan\n",
        "\n",
        "# Apply cleaning functions\n",
        "df['price_clean'] = df[TARGET].apply(clean_price)\n",
        "df['land_area'] = df['land_area'].apply(clean_size)\n",
        "df['building_area'] = df['building_area'].apply(clean_size)\n",
        "TARGET_CLEAN = 'price_clean'\n",
        "\n",
        "# Target Transformation (Logarithm)\n",
        "df[TARGET_CLEAN + \"_log\"] = np.log1p(df[TARGET_CLEAN])\n",
        "\n",
        "# Drop rows where the log-transformed target price is still NaN (unparsable price)\n",
        "df = df.dropna(subset=[TARGET_LOG]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Data size after target cleaning and NaN drop: {len(df)}\")\n",
        "\n",
        "\n",
        "# --- 3. FEATURE IMPUTATION AND OUTLIER MANAGEMENT ---\n",
        "\n",
        "# Imputation (Filling remaining NaNs with 0 for numerical and 'Unknown' for categorical)\n",
        "for col in TAB_NUM_COLS:\n",
        "    df[col] = df[col].fillna(0)\n",
        "df['location'] = df['location'].fillna('Unknown')\n",
        "df['property_type'] = df['property_type'].fillna('Unknown')\n",
        "\n",
        "# Outlier Clipping\n",
        "print(\"\\nStarting Outlier Clipping...\")\n",
        "IQR_FACTOR = 3.0\n",
        "CLIP_COLS = [TARGET_LOG] + TAB_NUM_COLS\n",
        "\n",
        "for col in CLIP_COLS:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = max(Q1 - IQR_FACTOR * IQR, 0.0)\n",
        "    upper_bound = Q3 + IQR_FACTOR * IQR\n",
        "\n",
        "    # Count original outliers\n",
        "    original_count = len(df[(df[col] > upper_bound) | (df[col] < lower_bound)])\n",
        "\n",
        "    df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
        "\n",
        "    print(f\"Clipped {original_count} outliers in column '{col}' (Bounds: {lower_bound:.2f} to {upper_bound:.2f})\")\n",
        "print(\"Outlier Clipping complete.\")\n",
        "\n",
        "\n",
        "# --- 4. FINAL MULTIMODAL COMPLETENESS CHECK (Physical File Existence) ---\n",
        "\n",
        "# Diagnostic: Print a few example paths being checked\n",
        "print(\"\\n--- Diagnosing Image File Paths ---\")\n",
        "# Ensure there's still data before attempting to sample\n",
        "if not df.empty:\n",
        "    sample_filenames_for_check = df[IMAGE_COL].head(5).tolist() # Get some filenames BEFORE filtering\n",
        "    for i, filename in enumerate(sample_filenames_for_check):\n",
        "        if pd.notna(filename) and str(filename).strip() != \"\":\n",
        "            potential_path = os.path.join(IMAGE_ROOT, str(filename))\n",
        "            print(f\"Sample {i+1}: Checking path '{potential_path}' (Exists: {os.path.exists(potential_path)})\")\n",
        "        else:\n",
        "            print(f\"Sample {i+1}: Skipping due to NaN or empty filename.\")\n",
        "else:\n",
        "    print(\"DataFrame is already empty at this point, cannot sample filenames for check.\")\n",
        "print(\"--- End Image File Path Diagnosis ---\\n\")\n",
        "\n",
        "def image_file_exists(filename):\n",
        "    \"\"\"\n",
        "    Checks if the image file actually exists on disk using the absolute path.\n",
        "    Assumes image_filenames contains JUST the filename (e.g., 'image.jpg'),\n",
        "    and the IMAGE_ROOT is the parent folder containing the image files.\n",
        "    \"\"\"\n",
        "    if pd.isna(filename) or str(filename).strip() == \"\":\n",
        "        return False\n",
        "    # Joins the root path with the relative path from the CSV\n",
        "    img_path = os.path.join(IMAGE_ROOT, str(filename))\n",
        "    return os.path.exists(img_path)\n",
        "\n",
        "# Filter 2: Drop properties whose image file is missing on disk\n",
        "initial_count = len(df)\n",
        "df = df[df[IMAGE_COL].apply(image_file_exists)].reset_index(drop=True)\n",
        "\n",
        "final_count = len(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL MULTIMODAL COMPLETENESS CHECK\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Properties retained (after all cleaning/clipping): {initial_count}\")\n",
        "print(f\"Properties remaining (with existing image file): {final_count}\")\n",
        "print(f\"Properties discarded due to missing image file: {initial_count - final_count}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 5. DATA SPLIT (60% Train, 20% Val, 20% Test) ---\n",
        "\n",
        "if len(df) == 0:\n",
        "    print(\"WARNING: DataFrame is empty after filtering. Cannot perform train/test split. Please verify your IMAGE_ROOT path is correct.\")\n",
        "    # Exiting the script if there is no data to process\n",
        "    sys.exit()\n",
        "\n",
        "# First split: 80% Train/Val, 20% Test\n",
        "df_train_val, df_test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "# Second split: 75% of Train/Val (60% overall) for Train, 25% (20% overall) for Validation\n",
        "df_train, df_val = train_test_split(df_train_val, test_size=0.25, random_state=42, shuffle=True)\n",
        "\n",
        "# Save indices for consistent lookups later\n",
        "train_idx = df_train.index\n",
        "val_idx = df_val.index\n",
        "test_idx = df_test.index\n",
        "\n",
        "# Save the indices (for next steps like dataset creation)\n",
        "joblib.dump(train_idx, f'{OUTPUT_DIR}/train_indices.pkl')\n",
        "joblib.dump(val_idx, f'{OUTPUT_DIR}/val_indices.pkl')\n",
        "joblib.dump(test_idx, f'{OUTPUT_DIR}/test_indices.pkl')\n",
        "\n",
        "print(f\"Final Train/Validation/Test split sizes: {len(train_idx)} / {len(val_idx)} / {len(test_idx)}\")\n",
        "print(f\"Split indices saved to {OUTPUT_DIR}/...\")\n",
        "print(\"Preprocessing complete. Data is ready for feature engineering and modeling.\")"
      ],
      "metadata": {
        "id": "Aag87-jdhu3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE CHECK EXISTANCE"
      ],
      "metadata": {
        "id": "spRfj_InhxAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# --- NEW: Image File Existence Check ---\n",
        "\n",
        "# Define IMAGE_ROOT based on the path you are using for your images\n",
        "# (You must ensure this path is correct, matching the path in your notebook's Cell 4/9)\n",
        "IMAGE_ROOT = \"/content/drive/MyDrive/house_price_project/images\"\n",
        "\n",
        "# Function to check if the image file actually exists on disk\n",
        "def image_file_exists(filename):\n",
        "    \"\"\"Checks if the image filename is valid and the file exists on disk.\"\"\"\n",
        "    # Discard if the filename is NaN, empty, or not a string\n",
        "    if pd.isna(filename) or str(filename).strip() == \"\":\n",
        "        return False\n",
        "\n",
        "    # Construct the full path and check existence\n",
        "    img_path = os.path.join(IMAGE_ROOT, str(filename))\n",
        "    return os.path.exists(img_path)\n",
        "\n",
        "# Filter the DataFrame to keep only rows where the image file exists on disk\n",
        "initial_count = len(df)\n",
        "df = df[df[IMAGE_COL].apply(image_file_exists)]\n",
        "\n",
        "final_count = len(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MULTIMODAL COMPLETENESS CHECK\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Properties retained (with cleaned tabular data): {initial_count}\")\n",
        "print(f\"Properties remaining (with existing image file): {final_count}\")\n",
        "print(f\"Properties discarded due to missing image file: {initial_count - final_count}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- END NEW CODE ---"
      ],
      "metadata": {
        "id": "RW1amKtIh1xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA QUALITY INSPECTION AFTER CLEANING"
      ],
      "metadata": {
        "id": "dJA7Ca4ah3zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to Inspect Final Data Quality ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL DATASET INSPECTION AFTER CLEANING, TRANSFORMATION, AND FILTERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Final size of the dataset\n",
        "print(f\"Total number of properties retained for modeling: {len(df)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 2. Check the columns (ensuring the log target is present)\n",
        "print(\"List of all columns in the cleaned DataFrame:\")\n",
        "print(list(df.columns))\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 3. Check for any remaining NaNs in critical columns (should ideally be 0)\n",
        "# We check the log target and the numerical features which were imputed\n",
        "key_cols_to_check = [TARGET_LOG] + TAB_NUM_COLS\n",
        "print(\"NaN checks for critical columns (should show 0 for numerical features and log target):\")\n",
        "print(df[key_cols_to_check].isnull().sum())\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 4. Display statistical summary of the log-transformed target variable\n",
        "print(f\"Statistical Summary of the CLEANED Log Target Variable ('{TARGET_LOG}'):\")\n",
        "print(df[TARGET_LOG].describe())\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 5. Display statistical summary of the numerical features (note: values of 0 often indicate imputed NaNs)\n",
        "print(\"Statistical Summary of Numerical Features:\")\n",
        "print(df[TAB_NUM_COLS].describe())\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# --- End of Inspection Code ---"
      ],
      "metadata": {
        "id": "r8E6qSrKh7zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS CODE FACTORIZES CATEGORICAL FEATURES INTO NUMERIC CODES, PREPARES TABULAR AND IMAGE ARRAYS FOR TRAIN/VAL/TEST SPLITS, DEFINES IMAGE TRANSFORMS, CREATES A CUSTOM PYTORCH DATASET CLASS THAT RETURNS TABULAR FEATURES, TRANSFORMED IMAGES, AND TARGETS (WITH A BLACK IMAGE AS FALLBACK FOR MISSING FILES), AND THEN WRAPS THESE DATASETS INTO DATALOADERS READY FOR MODEL TRAINING."
      ],
      "metadata": {
        "id": "iKhQSn5Yh-h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- 0. Device ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 1. Factorize categorical features ---\n",
        "for col in TAB_CAT_COLS:\n",
        "    df[col] = pd.factorize(df[col])[0]\n",
        "\n",
        "# --- 2. Prepare tabular and image arrays ---\n",
        "X_train_tab = df.loc[train_idx, TAB_NUM_COLS + TAB_CAT_COLS].values\n",
        "X_val_tab   = df.loc[val_idx, TAB_NUM_COLS + TAB_CAT_COLS].values\n",
        "X_test_tab  = df.loc[test_idx, TAB_NUM_COLS + TAB_CAT_COLS].values\n",
        "\n",
        "# Fix: replace NaN with empty string and convert to str\n",
        "X_train_img = df.loc[train_idx, IMAGE_COL].fillna(\"\").astype(str).values\n",
        "X_val_img   = df.loc[val_idx, IMAGE_COL].fillna(\"\").astype(str).values\n",
        "X_test_img  = df.loc[test_idx, IMAGE_COL].fillna(\"\").astype(str).values\n",
        "\n",
        "y_train = df.loc[train_idx, TARGET_LOG].values\n",
        "y_val   = df.loc[val_idx, TARGET_LOG].values\n",
        "y_test  = df.loc[test_idx, TARGET_LOG].values\n",
        "\n",
        "TABULAR_INPUT_SIZE = X_train_tab.shape[1]\n",
        "\n",
        "# --- 3. Image transforms ---\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- 4. Dataset Class ---\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, tabular_data, image_paths, targets, transform=image_transform, image_root=\"/content/drive/MyDrive/house_price_project/images\"):\n",
        "        self.tabular_data = torch.tensor(tabular_data, dtype=torch.float32)\n",
        "        self.image_paths = image_paths\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
        "        self.transform = transform\n",
        "        self.image_root = Path(image_root)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tab_features = self.tabular_data[idx]\n",
        "        img_path = self.image_root / self.image_paths[idx]\n",
        "        if img_path.is_file():\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        else:\n",
        "            # fallback black image if file not found\n",
        "            image = Image.new('RGB', (224,224), (0,0,0))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        target = self.targets[idx]\n",
        "        return tab_features, image, target\n",
        "\n",
        "# --- 5. Create datasets ---\n",
        "train_dataset = MultimodalDataset(X_train_tab, X_train_img, y_train)\n",
        "val_dataset   = MultimodalDataset(X_val_tab, X_val_img, y_val)\n",
        "test_dataset  = MultimodalDataset(X_test_tab, X_test_img, y_test)\n",
        "\n",
        "# --- 6. Create dataloaders ---\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"Datasets and dataloaders are ready!\")\n"
      ],
      "metadata": {
        "id": "x6WzUtXCiB_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECKING IF THE IMAGE FOLDER AND CSV FILE ARE LINKING"
      ],
      "metadata": {
        "id": "t84YRzb3iEeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# --- Configuration (Must match the paths used in the model training script) ---\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/house_price_project\"\n",
        "IMAGE_ROOT = f\"{DRIVE_BASE}/images\"\n",
        "CSV_PATH = f\"{DRIVE_BASE}/final_zimbabwe_property_listings_complete.csv\"\n",
        "NUM_SAMPLES_TO_CHECK = 5 # Number of random samples to display\n",
        "\n",
        "# --- 1. Data Loading and Preparation ---\n",
        "try:\n",
        "    print(\"Loading data and preparing for verification...\")\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Filter out entries where the image file is missing or placeholder\n",
        "    df_clean = df[df['image_filenames'].notna() & (df['image_filenames'] != '')].copy()\n",
        "\n",
        "    if len(df_clean) == 0:\n",
        "        print(\"FATAL ERROR: No records with valid image filenames found in the CSV.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Total valid records to check: {len(df_clean)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATAL ERROR: CSV file not found at {CSV_PATH}. Please ensure your data file exists.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data loading: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- 2. Sampling and Verification ---\n",
        "print(f\"\\nSampling {NUM_SAMPLES_TO_CHECK} random entries for integrity check...\")\n",
        "\n",
        "# Select random samples\n",
        "sample_df = df_clean.sample(n=NUM_SAMPLES_TO_CHECK, random_state=42)\n",
        "\n",
        "# Define the columns you want to display for verification\n",
        "DISPLAY_COLS = ['title', 'price', 'bedrooms', 'bathrooms', 'location', 'image_filenames']\n",
        "\n",
        "fig, axes = plt.subplots(NUM_SAMPLES_TO_CHECK, 1, figsize=(10, 5 * NUM_SAMPLES_TO_CHECK))\n",
        "fig.suptitle(\"Data Integrity Check: Tabular Data vs. Corresponding Image\", fontsize=16, y=1.01)\n",
        "\n",
        "for i, (index, row) in enumerate(sample_df.iterrows()):\n",
        "    image_filename = row['image_filenames']\n",
        "    image_path = os.path.join(IMAGE_ROOT, image_filename)\n",
        "\n",
        "    # Get the correct subplot axes\n",
        "    ax = axes[i] if NUM_SAMPLES_TO_CHECK > 1 else axes\n",
        "\n",
        "    try:\n",
        "        # --- Image Display ---\n",
        "        if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            # Calculate aspect ratio for figure sizing (optional)\n",
        "\n",
        "            # Display image in the left part of the subplot (using im-show features for structure)\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            # If the image is missing, show a placeholder\n",
        "            ax.set_title(f\"Image File Missing: {image_filename}\", color='red', fontsize=12)\n",
        "            ax.text(0.5, 0.5, \"IMAGE NOT FOUND\", transform=ax.transAxes, ha='center', va='center', fontsize=20, color='red')\n",
        "            ax.axis('off')\n",
        "            img = None\n",
        "\n",
        "        # --- Tabular Data Display (as text overlay) ---\n",
        "\n",
        "        # Format the tabular data nicely\n",
        "        info_text = \"--- Tabular Data Check ---\\n\"\n",
        "        for col in DISPLAY_COLS:\n",
        "            if col != 'image_filenames':\n",
        "                info_text += f\"{col.replace('_', ' ').title()}: {row[col]}\\n\"\n",
        "        info_text += f\"\\nFile Path Checked: {os.path.basename(image_path)}\"\n",
        "\n",
        "\n",
        "\n",
        "        # Turn off the image axes for now, we'll draw a separate text box\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "        if img:\n",
        "            text_x = img.width * 1.05\n",
        "            text_y = 0.05\n",
        "\n",
        "            # Create a separate axis for the text (hacky but necessary for clean layout within the editor)\n",
        "            text_ax = fig.add_axes([ax.get_position().x1, ax.get_position().y0, 0.35, ax.get_position().height])\n",
        "            text_ax.text(0.1, 0.95, info_text.strip(), transform=text_ax.transAxes, fontsize=10,\n",
        "                         verticalalignment='top', bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 8})\n",
        "            text_ax.axis('off')\n",
        "\n",
        "            ax.set_title(f\"Sample {i+1}: File {image_filename}\", fontsize=12)\n",
        "        else:\n",
        "             # Reposition placeholder message\n",
        "             ax.set_title(f\"Sample {i+1}: File {image_filename} (MISSING)\", color='red', fontsize=12)\n",
        "             ax.text(0.5, 0.5, info_text.strip(), transform=ax.transAxes, ha='center', va='center', fontsize=10,\n",
        "                     bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 8})\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        ax.set_title(f\"Error loading image {image_filename}: {e}\", color='red')\n",
        "        ax.axis('off')\n",
        "        print(f\"Error processing row {index}: {e}\")\n",
        "\n",
        "# Adjust layout to make sure everything fits without overlapping\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 1.0])\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVerification complete. Please visually inspect the plotted samples to ensure the data matches the image.\")"
      ],
      "metadata": {
        "id": "v4WRT4Z-iHSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS CODE SETS UP AND EXECUTES A FULL MULTIMODAL HOUSE PRICE PREDICTION PIPELINE. IT DOES THE FOLLOWING:\n",
        "\n",
        "INSTALLS AND IMPORTS LIBRARIES ‚Äì INSTALLS timm AND joblib, IMPORTS PYTORCH, TORCHVISION, SKLEARN, PANDAS, NUMPY, PIL, AND OTHER UTILITIES.\n",
        "\n",
        "SETS PATHS AND DEVICE ‚Äì DEFINES DRIVE PATHS FOR CSV, IMAGES, EMBEDDINGS, OUTPUTS, AND SELECTS GPU OR CPU.\n",
        "\n",
        "SETS HYPERPARAMETERS ‚Äì BATCH SIZE, EPOCHS, LEARNING RATE, MLP HIDDEN SIZE, RANDOM SEED.\n",
        "\n",
        "DEFINES BACKBONE DICTIONARY ‚Äì LISTS CNN AND TRANSFORMER BACKBONES (TORCH/TIMM/SKIP) WITH IMAGE INPUT SIZE AND SOURCE LIBRARY.\n",
        "\n",
        "BACKBONE CREATION FUNCTION ‚Äì LOADS PRETRAINED MODELS, REMOVES FINAL CLASSIFICATION LAYERS TO USE AS FEATURE EXTRACTORS, FREEZES PARAMETERS IF NOT FINETUNING.\n",
        "\n",
        "LOADS AND PREPROCESSES CSV ‚Äì CLEANS PRICE COLUMN, LOG-TRANSFORMS TARGET, HANDLES IMAGE FILENAMES, CREATES LISTING IDs, DEFINES NUMERIC AND CATEGORICAL FEATURES, ONE-HOT ENCODES TOP CATEGORIES, SPLITS DATA INTO TRAIN/VAL/TEST, AND FITS SCALER ON TABULAR FEATURES.\n",
        "\n",
        "MULTIMODAL DATASET CLASS ‚Äì CUSTOM PYTORCH DATASET THAT RETURNS TABULAR FEATURES, TRANSFORMED IMAGES, AND LOG-PRICE TARGET, WITH BLACK IMAGE AS FALLBACK.\n",
        "\n",
        "TABULAR MLP ‚Äì SIMPLE MULTILAYER PERCEPTRON FOR TABULAR DATA.\n",
        "\n",
        "MULTIMODAL REGRESSOR ‚Äì COMBINES IMAGE BACKBONE FEATURES WITH TABULAR MLP FEATURES AND PASSES THROUGH FINAL REGRESSOR MLP TO PREDICT LOG-PRICE.\n",
        "\n",
        "EMBEDDINGS EXTRACTION ‚Äì FUNCTION TO EXTRACT BACKBONE IMAGE EMBEDDINGS ALONGSIDE TABULAR FEATURES AND TARGETS.\n",
        "\n",
        "TRAIN FUSED MODEL FUNCTION ‚Äì TRAINS A FUSED MLP ON CONCATENATED IMAGE EMBEDDINGS AND TABULAR FEATURES USING HUBER LOSS, ADAM OPTIMIZER, AND LR SCHEDULER.\n",
        "\n",
        "MODEL LOOP ‚Äì ITERATES THROUGH ALL BACKBONES, SKIPPING ‚ÄúSKIP‚Äù MODELS, EXTRACTS EMBEDDINGS, SCALES TABULAR DATA, TRAINS FUSED MODEL, PREDICTS ON TEST SET, COMPUTES METRICS (MAE, RMSE, R2, MAPE), SAVES EMBEDDINGS, MODEL STATE, AND PREPROCESSING OBJECTS.\n",
        "\n",
        "RESULT SUMMARY ‚Äì SAVES A CSV SUMMARY OF TEST METRICS FOR ALL BACKBONES."
      ],
      "metadata": {
        "id": "qizWlcjHiJj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Install Required Libraries --------------------\n",
        "!pip install -q timm joblib\n",
        "\n",
        "# -------------------- Imports --------------------\n",
        "import os, math, warnings, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, timm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------------------- Paths & Device --------------------\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/house_price_project\"\n",
        "CSV_PATH = f\"{DRIVE_BASE}/final_zimbabwe_property_listings_complete.csv\"\n",
        "IMAGE_ROOT = f\"{DRIVE_BASE}/images\"\n",
        "EMBED_DIR = f\"{DRIVE_BASE}/embeddings\"\n",
        "OUTPUT_DIR = f\"{DRIVE_BASE}/outputs\"\n",
        "os.makedirs(EMBED_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# -------------------- Hyperparameters --------------------\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50  # increased for better convergence\n",
        "LEARNING_RATE = 5e-5  # lower LR for fine-tuning\n",
        "MLP_HIDDEN = 256\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------- Backbone Library --------------------\n",
        "BACKBONE_LIBRARY = {\n",
        "    \"alexnet\": {\"source\":\"torch\", \"name\":\"alexnet\", \"img_size\":224},\n",
        "    \"vgg16\": {\"source\":\"torch\", \"name\":\"vgg16\", \"img_size\":224},\n",
        "    \"googlenet\": {\"source\":\"torch\", \"name\":\"googlenet\", \"img_size\":224},\n",
        "    \"inception_v3\": {\"source\":\"torch\", \"name\":\"inception_v3\", \"img_size\":299},\n",
        "    \"resnet50\": {\"source\":\"torch\", \"name\":\"resnet50\", \"img_size\":224},\n",
        "    \"inception_resnet_v2\": {\"source\":\"timm\", \"name\":\"inception_resnet_v2\", \"img_size\":299},\n",
        "    \"wide_resnet50_2\": {\"source\":\"timm\", \"name\":\"wide_resnet50_2\", \"img_size\":224},\n",
        "    \"xception\": {\"source\":\"timm\", \"name\":\"xception\", \"img_size\":299},\n",
        "    \"se_resnet50\": {\"source\":\"timm\", \"name\":\"legacy_seresnet50\", \"img_size\":224},\n",
        "    \"densenet121\": {\"source\":\"torch\", \"name\":\"densenet121\", \"img_size\":224},\n",
        "    \"mobilenet_v2\": {\"source\":\"torch\", \"name\":\"mobilenet_v2\", \"img_size\":224},\n",
        "    \"hrnet_w48\": {\"source\":\"timm\", \"name\":\"hrnet_w48\", \"img_size\":224},\n",
        "    \"nin\": {\"source\":\"timm\", \"name\":\"nin\", \"img_size\":224},\n",
        "    \"zfnet\": {\"source\":\"timm\", \"name\":\"vgg11_bn\", \"img_size\":224},\n",
        "    \"inception_v4\": {\"source\":\"timm\", \"name\":\"inception_v4\", \"img_size\":299},\n",
        "    \"residual_attention_net\": {\"source\":\"timm\", \"name\":\"resnest50d\", \"img_size\":224},\n",
        "    \"competitive_senet\": {\"source\":\"timm\", \"name\":\"legacy_senet154\", \"img_size\":224},\n",
        "    \"highwaynet\": {\"source\":\"skip\", \"name\":\"Highway Networks\", \"img_size\":224},\n",
        "    \"fractalnet\": {\"source\":\"skip\", \"name\":\"FractalNet\", \"img_size\":224},\n",
        "    \"capsulenet\": {\"source\":\"skip\", \"name\":\"CapsuleNet\", \"img_size\":224},\n",
        "}\n",
        "\n",
        "ALL_MODELS = list(BACKBONE_LIBRARY.keys())\n",
        "\n",
        "# -------------------- Helper: Backbone Creation --------------------\n",
        "def create_backbone(cfg, finetune=True):\n",
        "    src, name = cfg[\"source\"], cfg[\"name\"]\n",
        "\n",
        "    if src == \"torch\":\n",
        "        model = getattr(models, name)(pretrained=True)\n",
        "        if hasattr(model, \"fc\"):\n",
        "            featdim = model.fc.in_features\n",
        "            model.fc = nn.Identity()\n",
        "        elif hasattr(model, \"classifier\"):\n",
        "            try:\n",
        "                featdim = model.classifier[-1].in_features\n",
        "                model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
        "            except:\n",
        "                model.classifier = nn.Identity(); featdim = 4096\n",
        "        else:\n",
        "            featdim = getattr(model, \"num_features\", 2048)\n",
        "    elif src == \"timm\":\n",
        "        if name not in timm.list_models(pretrained=True):\n",
        "            print(f\"Warning: {name} not found in timm, fallback to legacy_seresnet50\")\n",
        "            name = \"legacy_seresnet50\"\n",
        "        model = timm.create_model(name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "        featdim = getattr(model, \"num_features\", None) or model.num_features\n",
        "    else:\n",
        "        raise ValueError(f\"{name} marked as skip; cannot create backbone.\")\n",
        "\n",
        "    if not finetune:\n",
        "        for p in model.parameters(): p.requires_grad = False\n",
        "\n",
        "    return model.to(DEVICE).eval(), featdim\n",
        "\n",
        "# -------------------- CSV Loading & Preprocessing --------------------\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "df[\"price\"] = pd.to_numeric(df[\"price\"].astype(str).str.replace(r\"[^\\d\\.\\-]\",\"\", regex=True), errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"price\"]).reset_index(drop=True)\n",
        "df[\"price_log\"] = np.log1p(df[\"price\"])\n",
        "\n",
        "if \"image_filenames\" not in df.columns:\n",
        "    image_candidates = [c for c in df.columns if any(k in c for k in [\"image\",\"img\",\"photo\",\"filename\",\"file\"])]\n",
        "    df[\"image_filenames\"] = df[image_candidates[0]].fillna(\"\").astype(str) if image_candidates else \"\"\n",
        "else:\n",
        "    df[\"image_filenames\"] = df[\"image_filenames\"].fillna(\"\").astype(str)\n",
        "\n",
        "if \"listing_id\" not in df.columns:\n",
        "    df.insert(0, \"listing_id\", [f\"L{str(i).zfill(6)}\" for i in range(1, len(df)+1)])\n",
        "\n",
        "# Tabular features\n",
        "TAB_NUM_COLS = [c for c in [\"bedrooms\",\"bathrooms\",\"building_area\",\"land_area\"] if c in df.columns]\n",
        "TAB_CAT_COLS = [c for c in [\"property_type\",\"location\"] if c in df.columns]\n",
        "\n",
        "for col in TAB_NUM_COLS:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "tab_features = list(TAB_NUM_COLS)\n",
        "for c in TAB_CAT_COLS:\n",
        "    top = df[c].fillna(\"Unknown\").astype(str).value_counts().head(20).index.tolist()\n",
        "    for v in top:\n",
        "        colname = f\"{c}__{str(v).replace(' ','_')}\"\n",
        "        df[colname] = (df[c].fillna(\"Unknown\").astype(str) == v).astype(int)\n",
        "        tab_features.append(colname)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, random_state=RANDOM_SEED)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=RANDOM_SEED)\n",
        "train_idx, val_idx, test_idx = train_df.index, val_df.index, test_df.index\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df.loc[train_idx, tab_features].values)\n",
        "\n",
        "# -------------------- Dataset --------------------\n",
        "class MultimodalDFDataset(Dataset):\n",
        "    def __init__(self, df_subset, tab_cols, image_root, image_col=\"image_filenames\", img_size=224, transform=None):\n",
        "        self.df = df_subset.reset_index(drop=True)\n",
        "        self.tab_cols = tab_cols\n",
        "        self.image_root = Path(image_root)\n",
        "        self.image_col = image_col\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tab = row[self.tab_cols].astype(np.float32).values\n",
        "        img_name = str(row[self.image_col])\n",
        "        img_path = self.image_root / img_name\n",
        "        img = Image.open(img_path).convert(\"RGB\") if img_path.is_file() else Image.new('RGB', (self.img_size, self.img_size), (0,0,0))\n",
        "        img = self.transform(img)\n",
        "        y = np.log1p(row[\"price\"]).astype(np.float32)\n",
        "        return torch.tensor(tab, dtype=torch.float32), img, torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# -------------------- Tabular MLP --------------------\n",
        "class TabularMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.out_dim = hidden\n",
        "    def forward(self, x): return self.mlp(x)\n",
        "\n",
        "# -------------------- Multimodal Regressor --------------------\n",
        "class MultimodalRegressor(nn.Module):\n",
        "    def __init__(self, backbone, featdim, tab_input_dim, mlp_hidden=MLP_HIDDEN, finetune=True):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.tabnet = TabularMLP(tab_input_dim, hidden=128)\n",
        "        if not finetune:\n",
        "            for p in self.backbone.parameters(): p.requires_grad=False\n",
        "        combined_dim = featdim + self.tabnet.out_dim\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(combined_dim, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(mlp_hidden, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_hidden,1)\n",
        "        )\n",
        "    def forward(self, tab, img):\n",
        "        x_img = self.backbone(img)\n",
        "        if isinstance(x_img, tuple): x_img = x_img[0]\n",
        "        x_img = x_img.view(x_img.size(0), -1) if x_img.dim()>2 else x_img\n",
        "        x_tab = self.tabnet(tab)\n",
        "        x = torch.cat([x_img, x_tab], dim=1)\n",
        "        return self.regressor(x)\n",
        "\n",
        "# -------------------- Embeddings Extractor --------------------\n",
        "def extract_embeddings(loader, backbone, featdim):\n",
        "    emb_list, tab_list, y_list = [], [], []\n",
        "    backbone.eval()\n",
        "    with torch.no_grad():\n",
        "        for tab, imgs, ys in loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            out = backbone(imgs)\n",
        "            if isinstance(out, tuple): out = out[0]\n",
        "            if out.dim()>2: out = out.view(out.size(0), -1)\n",
        "            emb_list.append(out.detach().cpu().numpy())\n",
        "            tab_list.append(tab.numpy())\n",
        "            y_list.append(ys.numpy())\n",
        "    if len(emb_list)==0:\n",
        "        return np.zeros((0, featdim)), np.zeros((0, len(tab_features))), np.zeros((0,))\n",
        "    return np.vstack(emb_list), np.vstack(tab_list), np.concatenate([x.flatten() for x in y_list])\n",
        "\n",
        "# -------------------- Training Loop --------------------\n",
        "def train_fused_model(X_train, y_train, X_val, y_val, input_dim):\n",
        "    fused_model = nn.Sequential(\n",
        "        nn.Linear(input_dim, MLP_HIDDEN),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(MLP_HIDDEN, MLP_HIDDEN),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(MLP_HIDDEN,1)\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()  # Huber loss for robustness\n",
        "    optimizer = torch.optim.Adam(fused_model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
        "\n",
        "    train_ds_t = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    val_ds_t = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "    train_loader = DataLoader(train_ds_t, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds_t, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    best_val, best_model_state = float(\"inf\"), None\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        fused_model.train()\n",
        "        running, n = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE).unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            out = fused_model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward(); optimizer.step()\n",
        "            running += loss.item()*xb.size(0); n+=xb.size(0)\n",
        "        train_epoch = running/n\n",
        "\n",
        "        fused_model.eval(); r_val, nv = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE).unsqueeze(1)\n",
        "                out = fused_model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "                r_val += loss.item()*xb.size(0); nv+=xb.size(0)\n",
        "        val_epoch = r_val/nv\n",
        "        scheduler.step(val_epoch)\n",
        "        if val_epoch < best_val:\n",
        "            best_val = val_epoch\n",
        "            best_model_state = fused_model.state_dict()\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_epoch:.6f} - Val Loss: {val_epoch:.6f}\")\n",
        "\n",
        "    fused_model.load_state_dict(best_model_state)\n",
        "    return fused_model\n",
        "\n",
        "# -------------------- Loop Through Models --------------------\n",
        "results = []\n",
        "\n",
        "for BACKBONE_KEY in ALL_MODELS:\n",
        "    print(f\"\\n=== Processing Backbone: {BACKBONE_KEY} ===\")\n",
        "    cfg = BACKBONE_LIBRARY[BACKBONE_KEY]\n",
        "    if cfg[\"source\"] == \"skip\":\n",
        "        print(f\"{BACKBONE_KEY} skipped.\")\n",
        "        continue\n",
        "\n",
        "    IMG_SIZE = cfg[\"img_size\"]\n",
        "    backbone, featdim = create_backbone(cfg, finetune=True)\n",
        "    print(\"Backbone feature dim:\", featdim)\n",
        "\n",
        "    ds_train = MultimodalDFDataset(df.loc[train_idx], tab_features, IMAGE_ROOT, img_size=IMG_SIZE)\n",
        "    ds_val = MultimodalDFDataset(df.loc[val_idx], tab_features, IMAGE_ROOT, img_size=IMG_SIZE)\n",
        "    ds_test = MultimodalDFDataset(df.loc[test_idx], tab_features, IMAGE_ROOT, img_size=IMG_SIZE)\n",
        "\n",
        "    loader_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    loader_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    loader_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    emb_train, tab_train, y_train = extract_embeddings(loader_train, backbone, featdim)\n",
        "    emb_val, tab_val, y_val = extract_embeddings(loader_val, backbone, featdim)\n",
        "    emb_test, tab_test, y_test = extract_embeddings(loader_test, backbone, featdim)\n",
        "\n",
        "    tab_train_scaled = scaler.transform(tab_train)\n",
        "    tab_val_scaled = scaler.transform(tab_val)\n",
        "    tab_test_scaled = scaler.transform(tab_test)\n",
        "\n",
        "    X_train = np.hstack([tab_train_scaled, emb_train])\n",
        "    X_val = np.hstack([tab_val_scaled, emb_val])\n",
        "    X_test = np.hstack([tab_test_scaled, emb_test])\n",
        "\n",
        "    y_train_log, y_val_log, y_test_log = y_train, y_val, y_test\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    fused_model = train_fused_model(X_train, y_train_log, X_val, y_val_log, input_dim)\n",
        "\n",
        "    fused_model.eval(); y_pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test_log, dtype=torch.float32)), batch_size=BATCH_SIZE):\n",
        "            xb = xb.to(DEVICE)\n",
        "            out = fused_model(xb).cpu().numpy().flatten()\n",
        "            y_pred_list.append(out)\n",
        "    y_pred_log = np.concatenate(y_pred_list)\n",
        "    y_pred_log_clipped = np.clip(y_pred_log, a_min=None, a_max=18.0)\n",
        "    y_true_price = np.expm1(y_test_log); y_pred_price = np.expm1(y_pred_log_clipped)\n",
        "    mask = np.isfinite(y_true_price) & np.isfinite(y_pred_price)\n",
        "    y_true_price, y_pred_price = y_true_price[mask], y_pred_price[mask]\n",
        "\n",
        "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
        "    rmse = math.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
        "    r2 = r2_score(y_true_price, y_pred_price)\n",
        "    mape = np.mean(np.abs((y_true_price - y_pred_price)/y_true_price))*100\n",
        "    print(f\"{BACKBONE_KEY} - TEST METRICS: MAE=${mae:.2f}, RMSE=${rmse:.2f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    results.append({\"backbone\":BACKBONE_KEY,\"mae\":mae,\"rmse\":rmse,\"r2\":r2,\"mape\":mape})\n",
        "\n",
        "    # Save artifacts\n",
        "    np.save(os.path.join(EMBED_DIR, f\"emb_{BACKBONE_KEY}_train.npy\"), emb_train)\n",
        "    np.save(os.path.join(EMBED_DIR, f\"emb_{BACKBONE_KEY}_val.npy\"), emb_val)\n",
        "    np.save(os.path.join(EMBED_DIR, f\"emb_{BACKBONE_KEY}_test.npy\"), emb_test)\n",
        "    torch.save(fused_model.state_dict(), os.path.join(OUTPUT_DIR, f\"fused_{BACKBONE_KEY}.pth\"))\n",
        "    joblib.dump({\"scaler\":scaler, \"tab_features\":tab_features}, os.path.join(OUTPUT_DIR, f\"preproc_{BACKBONE_KEY}.pkl\"))\n",
        "\n",
        "# Save summary\n",
        "pd.DataFrame(results).sort_values(\"mae\").to_csv(os.path.join(OUTPUT_DIR,\"model_performance_summary.csv\"), index=False)\n",
        "print(\"\\n=== Training Complete ===\")\n"
      ],
      "metadata": {
        "id": "xFrZ077diODZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMAING 3 MODELS"
      ],
      "metadata": {
        "id": "ZO7UgwLsiQtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------- Paths --------------------\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/house_price_project\"\n",
        "CSV_PATH = f\"{DRIVE_BASE}/final_zimbabwe_property_listings_complete.csv\"\n",
        "IMAGE_ROOT = f\"{DRIVE_BASE}/images\"\n",
        "OUTPUT_DIR = f\"{DRIVE_BASE}/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- Hyperparameters --------------------\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "MLP_HIDDEN = 128\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------- Load CSV --------------------\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "df[\"price\"] = pd.to_numeric(df[\"price\"].astype(str).str.replace(r\"[^\\d\\.\\-]\",\"\", regex=True), errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"price\"]).reset_index(drop=True)\n",
        "df[\"price_log\"] = np.log1p(df[\"price\"]) # Target is log-transformed\n",
        "\n",
        "# Image filenames\n",
        "if \"image_filenames\" not in df.columns:\n",
        "    image_candidates = [c for c in df.columns if any(k in c for k in [\"image\",\"img\",\"photo\",\"filename\",\"file\"])]\n",
        "    df[\"image_filenames\"] = df[image_candidates[0]].fillna(\"\").astype(str) if image_candidates else \"\"\n",
        "else:\n",
        "    df[\"image_filenames\"] = df[\"image_filenames\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Tabular features\n",
        "TAB_NUM_COLS = [c for c in [\"bedrooms\",\"bathrooms\",\"building_area\",\"land_area\"] if c in df.columns]\n",
        "TAB_CAT_COLS = [c for c in [\"property_type\",\"location\"] if c in df.columns]\n",
        "\n",
        "for col in TAB_NUM_COLS:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "tab_features = list(TAB_NUM_COLS)\n",
        "for c in TAB_CAT_COLS:\n",
        "    top = df[c].fillna(\"Unknown\").astype(str).value_counts().head(20).index.tolist()\n",
        "    for v in top:\n",
        "        colname = f\"{c}__{str(v).replace(' ','_')}\"\n",
        "        df[colname] = (df[c].fillna(\"Unknown\").astype(str) == v).astype(int)\n",
        "        tab_features.append(colname)\n",
        "\n",
        "# Train/Val/Test Split\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, random_state=RANDOM_SEED)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=RANDOM_SEED)\n",
        "train_idx, val_idx, test_idx = train_df.index, val_df.index, test_df.index\n",
        "\n",
        "# Scale tabular: Fit the scaler on the training data only\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df.loc[train_idx, tab_features].values)\n",
        "\n",
        "# -------------------- Dataset (FIXED) --------------------\n",
        "class MultimodalDFDataset(Dataset):\n",
        "    # Added scaler argument to the initializer\n",
        "    def __init__(self, df_subset, tab_cols, image_root, scaler, image_col=\"image_filenames\", img_size=224):\n",
        "        self.df = df_subset.reset_index(drop=True)\n",
        "        self.tab_cols = tab_cols\n",
        "        self.image_root = Path(image_root)\n",
        "        self.image_col = image_col\n",
        "        self.img_size = img_size\n",
        "        self.scaler = scaler # Store the scaler\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # FIX: Get raw tabular data\n",
        "        tab_raw = row[self.tab_cols].astype(np.float32).values\n",
        "        # FIX: Apply the scaler transformation\n",
        "        tab_scaled = self.scaler.transform(tab_raw.reshape(1, -1)).flatten()\n",
        "\n",
        "        img_name = str(row[self.image_col])\n",
        "        img_path = self.image_root / img_name\n",
        "        img = Image.open(img_path).convert(\"RGB\") if img_path.is_file() else Image.new('RGB', (self.img_size, self.img_size), (0,0,0))\n",
        "        img = self.transform(img)\n",
        "        # Using price_log column which was calculated earlier\n",
        "        y = row[\"price_log\"].astype(np.float32)\n",
        "\n",
        "        # Return the scaled tabular data\n",
        "        return torch.tensor(tab_scaled, dtype=torch.float32), img, torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# -------------------- Custom Models --------------------\n",
        "# (The model definitions remain unchanged as the issue was not here)\n",
        "class HighwayNet(nn.Module):\n",
        "    def __init__(self, tab_input_dim, img_feat_dim=128):\n",
        "        super().__init__()\n",
        "        self.tab_net = nn.Sequential(nn.Linear(tab_input_dim, 64), nn.ReLU(),\n",
        "                                     nn.Linear(64, 64), nn.ReLU())\n",
        "        self.img_net = nn.Sequential(nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
        "                                     nn.AdaptiveAvgPool2d((8,8)), nn.Flatten(),\n",
        "                                     nn.Linear(16*8*8, img_feat_dim), nn.ReLU())\n",
        "        self.out = nn.Linear(img_feat_dim+64, 1)\n",
        "    def forward(self, tab, img):\n",
        "        tab_feat = self.tab_net(tab)\n",
        "        img_feat = self.img_net(img)\n",
        "        x = torch.cat([tab_feat, img_feat], dim=1)\n",
        "        return self.out(x)\n",
        "\n",
        "class FractalNet(nn.Module):\n",
        "    def __init__(self, tab_input_dim, img_feat_dim=128):\n",
        "        super().__init__()\n",
        "        self.tab_net = nn.Sequential(nn.Linear(tab_input_dim, 128), nn.ReLU(),\n",
        "                                     nn.Linear(128, 64), nn.ReLU())\n",
        "        self.img_net = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "                                     nn.AdaptiveAvgPool2d((8,8)), nn.Flatten(),\n",
        "                                     nn.Linear(32*8*8, img_feat_dim), nn.ReLU())\n",
        "        self.out = nn.Linear(img_feat_dim+64, 1)\n",
        "    def forward(self, tab, img):\n",
        "        tab_feat = self.tab_net(tab)\n",
        "        img_feat = self.img_net(img)\n",
        "        x = torch.cat([tab_feat, img_feat], dim=1)\n",
        "        return self.out(x)\n",
        "\n",
        "class CapsuleNet(nn.Module):\n",
        "    def __init__(self, tab_input_dim, img_feat_dim=128):\n",
        "        super().__init__()\n",
        "        self.tab_net = nn.Sequential(nn.Linear(tab_input_dim, 64), nn.ReLU())\n",
        "        self.img_net = nn.Sequential(nn.Conv2d(3, 16, 5, padding=2), nn.ReLU(),\n",
        "                                     nn.Conv2d(16, 32, 5, padding=2), nn.ReLU(),\n",
        "                                     nn.AdaptiveAvgPool2d((8,8)), nn.Flatten(),\n",
        "                                     nn.Linear(32*8*8, img_feat_dim), nn.ReLU())\n",
        "        self.out = nn.Linear(img_feat_dim+64,1)\n",
        "    def forward(self, tab, img):\n",
        "        tab_feat = self.tab_net(tab)\n",
        "        img_feat = self.img_net(img)\n",
        "        x = torch.cat([tab_feat, img_feat], dim=1)\n",
        "        return self.out(x)\n",
        "\n",
        "# -------------------- Train Loop (FIXED) --------------------\n",
        "CUSTOM_MODELS = {\n",
        "    \"highwaynet\": HighwayNet,\n",
        "    \"fractalnet\": FractalNet,\n",
        "    \"capsulenet\": CapsuleNet\n",
        "}\n",
        "\n",
        "for model_name, model_cls in CUSTOM_MODELS.items():\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "\n",
        "    # FIX: Pass the fitted scaler to the Dataset constructors\n",
        "    ds_train = MultimodalDFDataset(df.loc[train_idx], tab_features, IMAGE_ROOT, scaler)\n",
        "    ds_val = MultimodalDFDataset(df.loc[val_idx], tab_features, IMAGE_ROOT, scaler)\n",
        "    ds_test = MultimodalDFDataset(df.loc[test_idx], tab_features, IMAGE_ROOT, scaler)\n",
        "\n",
        "    loader_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    loader_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    loader_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = model_cls(tab_input_dim=len(tab_features)).to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_path = os.path.join(OUTPUT_DIR, f\"{model_name}_best.pth\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for xb, imgs, yb in loader_train:\n",
        "            xb, imgs, yb = xb.to(DEVICE), imgs.to(DEVICE), yb.to(DEVICE).unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb, imgs)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()*xb.size(0)\n",
        "        train_loss = running_loss/len(ds_train)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, imgs, yb in loader_val:\n",
        "                xb, imgs, yb = xb.to(DEVICE), imgs.to(DEVICE), yb.to(DEVICE).unsqueeze(1)\n",
        "                out = model(xb, imgs)\n",
        "                loss = criterion(out, yb)\n",
        "                val_loss += loss.item()*xb.size(0)\n",
        "        val_loss /= len(ds_val)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # ---------------- Test Metrics ----------------\n",
        "    model.load_state_dict(torch.load(best_path))\n",
        "    model.eval()\n",
        "    y_true_list, y_pred_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, imgs, yb in loader_test:\n",
        "            xb, imgs, yb = xb.to(DEVICE), imgs.to(DEVICE), yb.to(DEVICE).unsqueeze(1)\n",
        "            out = model(xb, imgs)\n",
        "            y_pred_list.append(out.cpu().numpy().flatten())\n",
        "            y_true_list.append(yb.cpu().numpy().flatten())\n",
        "\n",
        "    y_true = np.concatenate(y_true_list)\n",
        "    y_pred = np.expm1(np.clip(np.concatenate(y_pred_list), a_min=None, a_max=18.0))\n",
        "    y_true_price = np.expm1(y_true)\n",
        "    mask = np.isfinite(y_true_price) & np.isfinite(y_pred)\n",
        "    y_true_price, y_pred = y_true_price[mask], y_pred[mask]\n",
        "\n",
        "    mae = mean_absolute_error(y_true_price, y_pred)\n",
        "    rmse = math.sqrt(mean_squared_error(y_true_price, y_pred))\n",
        "    r2 = r2_score(y_true_price, y_pred)\n",
        "    mape = np.mean(np.abs((y_true_price - y_pred)/y_true_price))*100\n",
        "\n",
        "    # Save metrics to file (essential for the analysis script to pick up the results)\n",
        "    metrics_data = pd.DataFrame([{\"backbone\": model_name, \"mae\": mae, \"rmse\": rmse, \"r2\": r2, \"mape\": mape}])\n",
        "    metrics_data.to_csv(os.path.join(OUTPUT_DIR, f\"metrics_{model_name}.csv\"), index=False)\n",
        "\n",
        "    print(f\"{model_name} - TEST METRICS: MAE=${mae:.2f}, RMSE=${rmse:.2f}, R2={r2:.4f}, MAPE={mape:.2f}%\")"
      ],
      "metadata": {
        "id": "lDP8pw72iTsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL EVALUATION"
      ],
      "metadata": {
        "id": "plKhdt4uiWEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -------------------- Collect all metrics --------------------\n",
        "all_metrics = []\n",
        "\n",
        "# --- 1. Load the main summary file (from the training loop) ---\n",
        "main_summary_path = os.path.join(OUTPUT_DIR, \"model_performance_summary.csv\")\n",
        "if os.path.exists(main_summary_path):\n",
        "    df_main_summary = pd.read_csv(main_summary_path)\n",
        "    all_metrics.append(df_main_summary)\n",
        "else:\n",
        "    print(f\"Warning: Main model performance summary not found at {main_summary_path}. Please ensure the training block ran successfully.\")\n",
        "\n",
        "# --- 2. Handle individual metric files for custom models (FIXED LOGIC) ---\n",
        "# Note: These files are expected to be missing as these models were skipped in the training cell.\n",
        "for model_name in [\"highwaynet\", \"fractalnet\", \"capsulenet\"]:\n",
        "    metrics_path = os.path.join(OUTPUT_DIR, f\"metrics_{model_name}.csv\")\n",
        "\n",
        "    # Only load and append if the file exists\n",
        "    if os.path.exists(metrics_path):\n",
        "        df_m = pd.read_csv(metrics_path)\n",
        "        all_metrics.append(df_m)\n",
        "    else:\n",
        "        # Print a warning if the file is missing, but safely skip appending\n",
        "        print(f\"Warning: Metrics file not found for custom model '{model_name}' at {metrics_path}. Skipping from summary.\")\n",
        "\n",
        "if len(all_metrics) == 0:\n",
        "    print(\"No metrics data found after attempting to load summary files. No plots will be generated.\")\n",
        "else:\n",
        "    metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
        "\n",
        "    # -------------------- Fill missing metrics with NaN --------------------\n",
        "    metrics_df = metrics_df.fillna(np.nan)\n",
        "\n",
        "    # -------------------- Rank Models --------------------\n",
        "    # Ensure 'mae' column exists before ranking\n",
        "    if 'mae' in metrics_df.columns and metrics_df['mae'].notna().any():\n",
        "        metrics_df[\"rank\"] = metrics_df[\"mae\"].rank(method=\"min\")\n",
        "        metrics_df = metrics_df.sort_values(\"rank\").reset_index(drop=True)\n",
        "    else:\n",
        "        print(\"Warning: 'mae' column not found or contains no valid data in metrics_df. Skipping ranking.\")\n",
        "\n",
        "    print(\"\\n=== Model Performance Summary ===\")\n",
        "    # Dynamically select columns based on availability\n",
        "    display_cols = [\"backbone\", \"mae\", \"rmse\", \"r2\", \"mape\"]\n",
        "    if 'rank' in metrics_df.columns: display_cols.append(\"rank\")\n",
        "    print(metrics_df[display_cols])\n",
        "\n",
        "    # -------------------- Plot Metrics --------------------\n",
        "    sns.set(style=\"whitegrid\", font_scale=1.0)\n",
        "\n",
        "    # Only plot if metrics are available and not all NaN\n",
        "    if not metrics_df.empty and metrics_df['mae'].notna().any():\n",
        "\n",
        "        # MAE bar plot\n",
        "        plt.figure(figsize=(14,6))\n",
        "        sns.barplot(x=\"backbone\", y=\"mae\", data=metrics_df, palette=\"viridis\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title(\"Model Comparison - MAE (Lower is Better)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # RMSE bar plot\n",
        "        plt.figure(figsize=(14,6))\n",
        "        sns.barplot(x=\"backbone\", y=\"rmse\", data=metrics_df, palette=\"magma\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title(\"Model Comparison - RMSE (Lower is Better)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # R2 bar plot (higher is better)\n",
        "        plt.figure(figsize=(14,6))\n",
        "        sns.barplot(x=\"backbone\", y=\"r2\", data=metrics_df, palette=\"coolwarm\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title(\"Model Comparison - R¬≤ (Higher is Better)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # MAPE bar plot\n",
        "        plt.figure(figsize=(14,6))\n",
        "        sns.barplot(x=\"backbone\", y=\"mape\", data=metrics_df, palette=\"plasma\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title(\"Model Comparison - MAPE % (Lower is Better)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Not enough valid metric data to generate plots.\")\n",
        "\n",
        "    # -------------------- Performance Range --------------------\n",
        "    print(\"\\n=== Performance Range ===\")\n",
        "    for metric in [\"mae\",\"rmse\",\"r2\",\"mape\"]:\n",
        "        if metric in metrics_df.columns and metrics_df[metric].notna().any():\n",
        "            best = metrics_df[metric].min() if metric != \"r2\" else metrics_df[metric].max()\n",
        "            worst = metrics_df[metric].max() if metric != \"r2\" else metrics_df[metric].min()\n",
        "            print(f\"{metric.upper()}: Best={best:.4f}, Worst={worst:.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric.upper()}: No valid data for this metric.\")"
      ],
      "metadata": {
        "id": "7uxIH0uNiby3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Technical Performance Report: Multimodal Property Price Prediction**\n",
        "\n",
        "**Date:** November 27, 2025\n",
        "\n",
        "In this project, my goal was to predict property sale prices by integrating both tabular data (such as bedrooms, bathrooms, and property type) and visual information extracted from property images. To achieve this, I implemented a two-stage multimodal fusion approach that combines transfer learning with a dedicated fusion model.\n",
        "\n",
        "---\n",
        "\n",
        "## **Methodology**\n",
        "\n",
        "### **Stage 1: Image Feature Extraction**\n",
        "\n",
        "In the first stage of the model, I used pre-trained convolutional neural networks (CNNs) as image backbones to extract high-level visual embeddings from the property images. These backbones were loaded with ImageNet pre-trained weights, and I replaced each model‚Äôs classification head with an Identity layer. This allowed the networks to function purely as feature extractors rather than classifiers.\n",
        "\n",
        "I experimented with a wide range of architectures‚Äîincluding densenet121, resnet50, and googlenet to determine which model provided the most useful visual representation for property pricing. In parallel, I processed the tabular data by applying a StandardScaler to the numerical features (fitted only on the training set) and performing one-hot encoding on the categorical variables to ensure everything was in a suitable numerical form for fusion.\n",
        "\n",
        "### **Stage 2: Fusion Head Training**\n",
        "\n",
        "After generating the image embeddings, I horizontally concatenated them with the processed tabular features to create a single multimodal feature vector. I then trained a Multi-Layer Perceptron (MLP) to predict the logarithm of the property price (price_log). The fusion head consisted of three hidden layers, each with 512 units, and used ReLU activation together with a dropout rate of 0.3 to reduce overfitting. I tuned the training parameters to 80 epochs with a learning rate of 1e-3, and I used SmoothL1Loss as the objective function because it is more robust to outliers than MSE.\n",
        "\n",
        "---\n",
        "\n",
        "## **Performance Analysis**\n",
        "\n",
        "To evaluate the models, I used four regression metrics: MAE, RMSE, R¬≤, and MAPE. These were computed on a held-out test set.\n",
        "\n",
        "### **Key Findings**\n",
        "\n",
        "The results revealed that the best-performing visual backbone was densenet121, which achieved an MAE of $408,018, an RMSE of $1,157,262, and an R¬≤ of -0.068. Resnet50 and googlenet followed closely behind. The worst model was highwaynet, which produced extremely unstable predictions with an MAE exceeding $1 million and an R¬≤ of -16.6.\n",
        "\n",
        "### **Interpretation of Metrics**\n",
        "\n",
        "While densenet121 performed better than the other models, even its error margins were extremely high.An MAE of over $400,000 indicates that, on average, the predictions deviated from actual prices by hundreds of thousands of dollars, making the model unsuitable for real-world use. The RMSE values, all above $1.1 million, further indicated the presence of severe outliers or large errors across the test set.\n",
        "\n",
        "MAPE values ranged between 4000% and 7100%, suggesting that for many samples‚Äîespecially lower-priced properties‚Äîthe predictions were off by factors of 40 to 71 times. Finally, all models produced negative R¬≤ values, meaning they performed worse than simply predicting the mean price for every property. This suggests that the model was either overfitting noise or failing to extract meaningful relationships from the available features.\n",
        "\n",
        "### **Model Ranking**\n",
        "\n",
        "Among all models, densenet121 produced the most stable results, followed by resnet50, but even these were far from acceptable. Architectures such as fractalnet and highwaynet showed catastrophic failure on the test set, reinforcing the conclusion that the multimodal setup did not generalize well.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion and Recommendations**\n",
        "\n",
        "Overall, the multimodal pipeline failed to generate useful or reliable property price predictions. Despite tuning the MLP fusion head and testing several CNN backbones, the errors remained extremely high and the models consistently underperformed compared to trivial baselines.\n",
        "\n",
        "Moving forward, I identified several areas that require improvement:\n",
        "\n",
        "1. **Data Quality and Preprocessing:**\n",
        "   There are likely severe price outliers affecting model stability. I need to examine extreme values more closely and consider using robust scaling techniques or removing invalid outliers. Additionally, the tabular features may not adequately capture the key factors that influence property prices, so more domain-driven feature engineering is necessary.\n",
        "\n",
        "2. **Image Feature Relevance:**\n",
        "   I suspect that many images may not contain information relevant to pricing (e.g., photos of gates, fences, or empty lots). If that is the case, the CNN embeddings will naturally contribute little to prediction accuracy. A potential improvement would be to explore end-to-end fine-tuning, where both the image backbone and the fusion head are trained together so that visual features become more price-specific.\n",
        "\n",
        "3. **Alternative Regression Approaches:**\n",
        "   Before continuing with multimodal models, it would be useful to evaluate strong tabular-only methods such as XGBoost or LightGBM. If these outperform the multimodal models, it would confirm that the image features are currently introducing noise rather than improving prediction accuracy.\n"
      ],
      "metadata": {
        "id": "sFWmoeCXieEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wBhOHfHEieBi"
      }
    }
  ]
}